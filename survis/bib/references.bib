
@misc{mettes_hyperbolic_2023,
	title = {Hyperbolic {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	shorttitle = {Hyperbolic {Deep} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2305.06611},
	doi = {10.48550/arXiv.2305.06611},
	abstract = {Deep representation learning is a ubiquitous part of modern computer vision. While Euclidean space has been the de facto standard manifold for learning visual representations, hyperbolic space has recently gained rapid traction for learning in computer vision. Speciﬁcally, hyperbolic learning has shown a strong potential to embed hierarchical structures, learn from limited samples, quantify uncertainty, add robustness, limit error severity, and more. In this paper, we provide a categorization and in-depth overview of current literature on hyperbolic learning for computer vision. We research both supervised and unsupervised literature and identify three main research themes in each direction. We outline how hyperbolic learning is performed in all themes and discuss the main research problems that beneﬁt from current advances in hyperbolic learning for computer vision. Moreover, we provide a highlevel intuition behind hyperbolic geometry and outline open research questions to further advance research in this direction.},
	language = {en},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Mettes, Pascal and Atigh, Mina Ghadimi and Keller-Ressel, Martin and Gu, Jeffrey and Yeung, Serena},
	month = may,
	year = {2023},
	note = {arXiv:2305.06611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/50/Mettes 等 - 2023 - Hyperbolic Deep Learning in Computer Vision A Survey.pdf:application/pdf},
}

@article{akbar_comprehensive_2024,
	title = {A {Comprehensive} {Review} on {Deep} {Learning} {Assisted} {Computer} {Vision} {Techniques} for {Smart} {Greenhouse} {Agriculture}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10379667/},
	doi = {10.1109/ACCESS.2024.3349418},
	abstract = {With the escalating global challenges of food security and resource sustainability, innovative solutions like deep learning and computer vision are transforming agricultural practices by enabling data-driven decision-making. This paper provides a focused review of recent advancements in deep learning-enabled computer vision techniques tailored specifically for greenhouse environments. First, deep learning and computer vision fundamentals are briefly introduced. Over 100 studies from 2020 to date are then comprehensively reviewed in which these technologies were applied within greenhouses for growth monitoring, disease detection, yield estimation, and other tasks. The techniques, datasets, models, and overall performance results reported in the literature are analyzed. Tables and figures showcase real-world implementations and results synthesized from current research. Key challenges are also outlined related to aspects like model adaptability, lack of sufficient labeled greenhouse data, computational constraints, the need for multi-modal sensor fusion, and other areas needing further investigation. Future trends and prospects are discussed to provide guidance for researchers exploring computer vision in the niche greenhouse domain. By condensing prior work and elucidating the state-of-the-art, this timely review aims to promote continued progress in smart greenhouse agriculture. The focused analysis, specifically on greenhouse environments, fills a gap compared to previous agricultural surveys. Overall, this paper highlights the immense potential of computer vision and deep learning in driving the emergence of data-driven, smart greenhouse farming worldwide.},
	language = {en},
	urldate = {2025-05-09},
	journal = {IEEE Access},
	author = {Akbar, Jalal Uddin Md and Kamarulzaman, Syafiq Fauzi and Muzahid, Abu Jafar Md and Rahman, Md. Arafatur and Uddin, Mueen},
	year = {2024},
	pages = {4485--4522},
	file = {PDF:files/51/Akbar 等 - 2024 - A Comprehensive Review on Deep Learning Assisted Computer Vision Techniques for Smart Greenhouse Agr.pdf:application/pdf},
}

@article{manakitsa_review_2024,
	title = {A {Review} of {Machine} {Learning} and {Deep} {Learning} for {Object} {Detection}, {Semantic} {Segmentation}, and {Human} {Action} {Recognition} in {Machine} and {Robotic} {Vision}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/12/2/15},
	doi = {10.3390/technologies12020015},
	abstract = {Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications.},
	language = {en},
	number = {2},
	urldate = {2025-05-09},
	journal = {Technologies},
	author = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and Fragulis, George F.},
	month = jan,
	year = {2024},
	pages = {15},
	file = {PDF:files/52/Manakitsa 等 - 2024 - A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Huma.pdf:application/pdf},
}

@misc{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	doi = {10.48550/arXiv.2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	language = {en},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = nov,
	year = {2020},
	note = {arXiv:2001.05566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:files/53/Minaee 等 - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf},
}

@misc{zheng_deep_2024,
	title = {Deep {Learning} for {Event}-based {Vision}: {A} {Comprehensive} {Survey} and {Benchmarks}},
	shorttitle = {Deep {Learning} for {Event}-based {Vision}},
	url = {http://arxiv.org/abs/2302.08890},
	doi = {10.48550/arXiv.2302.08890},
	abstract = {Event cameras are bio-inspired sensors that capture the per-pixel intensity changes asynchronously and produce event streams encoding the time, pixel position, and polarity (sign) of the intensity changes. Event cameras possess a myriad of advantages over canonical frame-based cameras, such as high temporal resolution, high dynamic range, low latency, etc. Being capable of capturing information in challenging visual conditions, event cameras have the potential to overcome the limitations of frame-based cameras in the computer vision and robotics community. In very recent years, deep learning (DL) has been brought to this emerging field and inspired active research endeavors in mining its potential. However, there is still a lack of taxonomies in DL techniques for event-based vision. We first scrutinize the typical event representations with quality enhancement methods as they play a pivotal role as inputs to the DL models. We then provide a comprehensive survey of existing DL-based methods by structurally grouping them into two major categories: 1) image/video reconstruction and restoration; 2) event-based scene understanding and 3D vision. We conduct benchmark experiments for the existing methods in some representative research directions i.e., image reconstruction, deblurring, and object recognition, to identify some critical insights and problems. Finally, we have discussions regarding the challenges and provide new perspectives for inspiring more research studies.},
	language = {en},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Zheng, Xu and Liu, Yexin and Lu, Yunfan and Hua, Tongyan and Pan, Tianbo and Zhang, Weiming and Tao, Dacheng and Wang, Lin},
	month = apr,
	year = {2024},
	note = {arXiv:2302.08890 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/54/Zheng 等 - 2024 - Deep Learning for Event-based Vision A Comprehensive Survey and Benchmarks.pdf:application/pdf},
}

@inproceedings{chollet_xception_2017,
	address = {Honolulu, HI},
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Xception},
	url = {http://ieeexplore.ieee.org/document/8099678/},
	doi = {10.1109/CVPR.2017.195},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efﬁcient use of model parameters.},
	language = {en},
	urldate = {2025-05-09},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chollet, Francois},
	month = jul,
	year = {2017},
	pages = {1800--1807},
	file = {PDF:files/55/Chollet - 2017 - Xception Deep Learning with Depthwise Separable Convolutions.pdf:application/pdf},
}

@article{amjoud_object_2023,
	title = {Object {Detection} {Using} {Deep} {Learning}, {CNNs} and {Vision} {Transformers}: {A} {Review}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	shorttitle = {Object {Detection} {Using} {Deep} {Learning}, {CNNs} and {Vision} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/10098596/},
	doi = {10.1109/ACCESS.2023.3266093},
	abstract = {Detecting objects remains one of computer vision and image understanding applications’ most fundamental and challenging aspects. Significant advances in object detection have been achieved through improved object representation and the use of deep neural network models. This paper examines more closely how object detection has evolved in the era of deep learning over the past years. We present a literature review on various state-of-the-art object detection algorithms and the underlying concepts behind these methods. We classify these methods into three main groups: anchor-based, anchor-free, and transformerbased detectors. Those approaches are distinct in the way they identify objects in the image. We discuss the insights behind these algorithms and experimental analyses to compare quality metrics, speed/accuracy tradeoffs, and training methodologies. The survey compares the major convolutional neural networks for object detection. It also covers the strengths and limitations of each object detector model and draws significant conclusions. We provide simple graphical illustrations summarising the development of object detection methods under deep learning. Finally, we identify where future research will be conducted.},
	language = {en},
	urldate = {2025-05-09},
	journal = {IEEE Access},
	author = {Amjoud, Ayoub Benali and Amrouch, Mustapha},
	year = {2023},
	pages = {35479--35516},
	file = {PDF:files/56/Amjoud和Amrouch - 2023 - Object Detection Using Deep Learning, CNNs and Vision Transformers A Review.pdf:application/pdf},
}

@article{esteva_deep_2021,
	title = {Deep learning-enabled medical computer vision},
	volume = {4},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00376-2},
	doi = {10.1038/s41746-020-00376-2},
	abstract = {Abstract
            A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.},
	language = {en},
	number = {1},
	urldate = {2025-05-09},
	journal = {npj Digital Medicine},
	author = {Esteva, Andre and Chou, Katherine and Yeung, Serena and Naik, Nikhil and Madani, Ali and Mottaghi, Ali and Liu, Yun and Topol, Eric and Dean, Jeff and Socher, Richard},
	month = jan,
	year = {2021},
	pages = {5},
	file = {PDF:files/57/Esteva 等 - 2021 - Deep learning-enabled medical computer vision.pdf:application/pdf},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	language = {en},
	number = {1},
	urldate = {2025-05-09},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = dec,
	year = {2019},
	pages = {60},
	file = {PDF:files/58/Shorten和Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learning.pdf:application/pdf},
}

@misc{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	url = {http://arxiv.org/abs/1703.04977},
	doi = {10.48550/arXiv.1703.04977},
	abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. Traditionally it has been difﬁcult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the beneﬁts of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
	language = {en},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Kendall, Alex and Gal, Yarin},
	month = oct,
	year = {2017},
	note = {arXiv:1703.04977 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/59/Kendall和Gal - 2017 - What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:application/pdf},
}
